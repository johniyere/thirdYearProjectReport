\chapter{Recommender System} \label{chap:Recommender}

This chapter will discuss the machine learning approach to building a Recommender system proposed in \autoref{sec:RecSystems}. It discusses the different approaches to creating this Recommender system and explains the reasons behind the chosen method.

\section{Information Gathering}
The Recommender System requires a representation of a users preferences in a format suitable for statistical analysis. To generate this representation, we need to collect some form of feedback from the users. There are three types of feedback that can be retrieved from the user.

\subsubsection{Implicit Feedback}
The user preferences are attained automatically from \gls{implicit} user actions on the system.This reduces the amount of work and input needed by the user, however it is less accurate. Implicit data we can collect is the number of times a user clicks on a trail, but a user clicking on a trail does not reflect their feelings towards that trail.

\subsubsection{Explicit Feedback}
The system requires users to reflect on the choices of trails that they make. In our system we provide a review system discussed in \autoref{subsec:trailDescription}, that asks users to rate the trails that they've run \cite{jawaheer2010comparison}. As the users provides this data explicitly, it is more accurate and more greatly reflects the users preferences. However, users are less likely to leave reviews and ratings unless forced too.

\subsubsection{Hybrid Feedback}
A combination of both Implicit and Explicit feed-backs, using the strengths of both and minimising the weakness of each feedback method. 

Due to the inaccuracy of implicit feedback, especially in our scenario, I decided to just rely on explicit feedback.

\section{Choosing A Recommender Algorithms} \label{chooseRecAlg}
There are multiple different ways of implementing Recommender system. For a machine learning approach there, there are three popular approaches used which are
\begin{itemize}
    \item Content-based filtering
    \item Collaborative filtering
    \item Hybrid Technique
\end{itemize}
as shown in figure \ref{fig:recommenderSystemTypes}. The main feature that the approaches listed above is similarity.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{recommenderSystemTypes.jpg}
    \caption{Types of Recommender Systems}
    \label{fig:recommenderSystemTypes}
\end{figure}

\subsubsection{Content-based filtering}
\acrfull{cbf} recommends items based on information known about the particular item \cite{pazzani2007content}. The system would have some meta-data about the item, for example, in our scenario we would have extra information about trails such as the type of terrain, location, elevation etc. Items are recommended to the user based on features extracted from users history of positively rated items that represents a users preferences \cite{isinkaye2015recommendation}.

\acrshort{cbf} resolves some of the problems of \acrshort{cf}. The system can still recommend items to users who have not rated any items \cite{burke2002hybrid}. However the the strength of the system greatly relies on how well the content analysis is. 

Content Analysis is the technique of mapping symbolic data into data in a matrix format suitable for analysis \cite{roberts2001content}. For some domains such as news and publications it is quite effective, but for user data that is quite subjective and can be influenced by a large number of variables, it can be difficult and the attributes retrieved may not be suitable to categorise a trail. Having the user add attributes to trails is not a very user friendly method as users are not inclined to perform such menial tasks.
 \ref{fig:contentBasedFiltering}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{content-based-filtering.png}
    \caption{Content based filtering}
    \label{fig:contentBasedFiltering}
\end{figure}

\subsubsection{Collaborative filtering}
\acrfull{cf} is a domain independent technique especially useful when content analysis of the items cannot be easily done \cite{isinkaye2015recommendation}. It does so by matching users that are similar based on the user preferences and using this matching to make recommendations \cite{herlocker2004evaluating}. The idea is that, user A likes routes 1 \& 2 and user B likes routes 2 \& 3. As user A and user B like the same route (route 2), then they are similar and user A would also like route 3. 

This property can be seen in real life and works extremely well. People are more likely to trust the suggestions of their friends, i.e. people that they are the most similar with. With this system, you do not need to know any extra information on the routes you are running the algorithm on. We us either explicit information such as user ratings, or implicit information such as number of views on a route, to determine the routes users like. Example shown in figure \ref{fig:collaborativeFiltering}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{collaborative-filtering.png}
    \caption{Collaborative filtering}
    \label{fig:collaborativeFiltering}
\end{figure}

As trails are quite hard item to perform content analysis on, I decided to chose the collaborative filtering technique. It is best suited to trail running as users are more likely to run routes recommended to them by others.

\subsubsection{Hybrid Recommender Systems}
Hybrid Recommender systems simply combine the results of both collaborative and content-based techniques \cite{claypool1999combing}. The results from both the system's would then have to be ranked again in the combiner as shown in figure \ref{fig:hybridRecommenderSystems}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{hybrid-approaches.jpg}
    \caption{Hybrid Approaches}
    \label{fig:hybridRecommenderSystems}
\end{figure}

Although this may be a more appropriate approach to building Recommender Systems, it still requires the complexities from content-based filtering, hence this approach was not used. For this reason, we use a \acrlong{cf} Technique.

\section{Collaborative Filtering}
Collaborative filtering techniques require the system to users that are similar based on their user preferences inferred from the ratings they give trails they have run previously. There are 2 main approaches to collaborative filtering. We compare these methods below to decide which method is is best suited to our application.

\subsubsection{Memory based Technique}
Memory based
With memory based techniques, we calculate similarity scores either between users or between items based on the ratings \cite{wang2006unifying}. 

The problem with this method is that it has to be run every time the user queries the system. Since there is no model to train as with most machine learning methods, you will have to calculate the similarity score with all the items each time which is a huge performance bottle neck.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{collab-filtering-types.png}
    \caption{Memory based tachniques ti collaborative filtering}
    \label{fig:collabFilteringTypes}
\end{figure}

\subsubsection{Model based techniques}
To improve on the issues of the memory based techniques, we can use model based techniques. We create a machine learning model that will be trained on a dataset of user-routes ratings using the Matrix Factorization method described in section \ref{matrixFactorization}. We will use a neural network described in section \ref{neuralNetwork}
\section{Matrix Factorization} \label{matrixFactorization}
For our Machine learning model we want to learn two features. 
\begin{itemize}
    \item what properties a specific user likes in routes
    \item what properties a route has that a route has
\end{itemize}
We can learn these features using a method called Matrix Factorization, which is one of the top methods proposed during the Netflix Prize \cite{bell2007lessons}.

Matrix Factorization is the process of decomposing a user-route rating matrix into a product of 2 lower dimensionality rectangular matrices \cite{koren2009bellkor} as shown in figure \ref{fig:matrixFactorization}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{matrix-factorization.png}
    \caption{Example of Matrix Factorization}
    \label{fig:matrixFactorization}
\end{figure}

We can represent the above mathematically as
\begin{equation}
    R \approx P \times Q^T
\end{equation}

Given the type of rating matrix shown in figure \ref{matrixFactorization}, we want to predict what a user will rate the other items that they have not rate yet. This will allow us to predict what item's a user likes and suggest it to the user. What we try to do with Matrix factorization is learn the reason why users have rated the items that they have rated (assuming a user only rates items they use). These reasons are called Latent features\footnote{Latent is a synonym of Hidden}.

These features are latent because we do not initially know them, we don't even know if they exist. Our decomposed features are initialized to random values, and then we use a method called Gradient Decent to try and find what those values should be.

\subsection{Gradient Descent}
With our decomposed feature matrices, we want to optimize those values such that the product will be very similar to the original ratings matrix. We can do this using an optimization algorithm called Gradient Descent \cite{ruder2016overview}



\subsubsection{Loss Function}
With our training set, we know what the values of certain user-route ratings should be. When the predict the values of a features for these known values, we can calculate a predicted value for the user-route rating\footnote{using \url{http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/} for help here}

\begin{equation}
    \hat{r}_{ij} = {p^{T}_{j}}{q_j}=\sum_{k=1}^{k}{p_{ik}q_{ik}}
\end{equation}

With this predicted value, we can then calculate what is called our loss value. That is how wrong we are from the actual predicted value, we can calculate our loss value using the Mean Squared Error (MSE) Loss Function as shown in equation \ref{eqn:mseLossFunction}

\begin{equation} \label{eqn:mseLossFunction}
    MSE = \frac{1}{N}\sum_{i=1}^{N}{({f_i}-{y_i})}^2 
\end{equation}

What our gradient descent optimization function does is to try and minimize this values. By taking learning steps towards a minimum value similar to the image shown in figure \ref{fig:gradientDescent}. With gradient descent we back propagate these changes that are representatives as derivatives and change our initial feature matrix values and then run the method again, trying to further optimize our values. The gradient descent function that we use is call Adam \cite{kingma2014adam}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{gradient-descent.png}
    \caption{Gradient Descent}
    \label{fig:gradientDescent}
\end{figure}

\subsection{Bias is a virtue}
To improve our model, we can introduce bias. In real life, you would trust a user that has run more routes than a user that has run fewer routes. This also applies for routes that have been run by more users. We can include this feature into our model to help improve the model created by adding bias values related to each user and each item (route). We include this bias in when training our model.

\begin{equation}
    \hat{r_{ui}} = \mu + b_i + b_u + {q^{T}_{i}}{p_u}
\end{equation}

\section{A Deep Learning Approach} \label{neuralNetwork}
Neural networks are modelled after the brains biological networks \cite{van2018artificial}. We can create our model discussed in section \ref{matrixFactorization}, using a neural network which is generally more Robust. Our neural network will use the initial feature matrices as our input to the neural networks. The hidden layer will consist of one dense layer with a ReLU activation function \cite{li2017convergence}, and then have a final output layer with one node. We use the same MSE as discussed before and use back-propagation to update our weights.

With the neural network, we can also add bias to help improve the model.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{neural-net-example.jpg}
    \caption{Neural Network Example}
    \label{fig:neuralNetworkExample}
\end{figure}

\subsection{Preventing overfitting}
One of the main problems with machine learning approaches is over-fitting and under fitting.

\paragraph{Overfitting} is when our model is trained to match to perfectly with our training dataset and therefore has a hard time to predict when we have a completely new data. To prevent this in Neural Networks. Drop out layers kill random nodes in our neural network during each epoch. This forces the neural network to always try and learn new paths to during training preventing it form relying on the same weights each time.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{fitting-problem.png}
    \caption{Fitting Problems}
    \label{fig:fittingProblems}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{dropout.png}
    \caption{Dropout Layer}
    \label{fig:dropoutLayer}
\end{figure}
\section{Tensorflow and Python}
There are a few machine learning platforms that can help us to create our neural network. I decided to use tensorflow because it is popular and has a big community to help. It also is built with keras which allows you to create these models from a High Level. Tensor flow also has C++ bindings that takes use of the GPU for the matrix which is far more efficient than using a normal CPU and so is quicker for handling large datasets.

Tensorflow is mainly meant to be used in python as pytbon is the main language for Data Science. This proved to be a problem initially as I did not know python. Although Tensorflow also provides a javascript library\footnote{https://www.tensorflow.org/js}, I chose to use the python version as the javascript library did not provide all the functionality of the main python library. It also allowed me to use Jupyter Notebook (discussed in section \ref{jupyterNotebook}) which provided a really platform to experiment with the machine learning model.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{neural-net-summary.png}
    \caption{Summary of Neural Network}
    \label{fig:neuralNetworkSummary}
\end{figure}

\subsection{Jupyter Notebook} \label{jupyterNotebook}
Jupyter Notebook\footnote{https://jupyter.org/} that allows creating and running notebooks via a web appplication. These documents can contain both Markup text and more importantly code that is exists in cells on the document. The document allows you to easily share any code with others easily as it make's it straightforward to document. With it's use of cells, you can rerun individual blocks of code with running the entire document, making it easy to develop and iterate through different versions as you improve.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{jupyter-notebook.png}
    \caption{Jupyter Notebook}
    \label{fig:JupyterNotebook}
\end{figure}

